{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Lamarckism\n",
    "\n",
    "This notebook as a couple of experiments around the idea of adding layers to an existing model and then train it some more in hopes this will improve the accuracy of the model. \n",
    "\n",
    "The experiments are borrowed from [this blogpost](https://myrtle.ai/how-to-train-your-resnet-4-architecture/) ([notebook](https://github.com/davidcpage/cifar10-fast/blob/master/experiments.ipynb)) in which the author sets out to achieve 94% accuracy on CIFAR10 in as short time as possible. Apart from the overall usefulness of fast training, the experiments feature a backbone model which achieves just over 90% accuracy which is then enhanced with a few more layers to achive 94% accuracy when trained from scratch.\n",
    "\n",
    "This will serve as a baseline for the experiments in this notebook. Instead of creating a new model from scratch, we here will try to add the extra layers to an already trained backbone model and then try to train it to achive the same accuracy as if the larger model was fully trained from scratch.\n",
    "\n",
    "Summary of experiments is that things are a bit less straightforward than I though, but the concept seems to be useful. It is quite difficult to improve a converged model without any temporary decrease in accuracy, but if the changes are made before the model has converged it seems to have less impact.\n",
    "\n",
    "This could indicate that some kind of annealing of the mutation rate is useful, or else almost converged models might see too much perturbation to converge efficiently. \n",
    "\n",
    "This is a bit unfortunate as it brings in a need to determine whether models have converged or not and this might not be easy to do well without prior knowledge of how fitting a particular data set plays out.\n",
    "\n",
    "Another learning is that it seems to be important to keep the learning rate relatively high, possibly as this pushes the model out of whatever low loss region it is in to somewhere else where the new layers are actually used for something.\n",
    "\n",
    "Stay tuned for when some other data set or training method completely invalidates everything in this notebook! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling NaiveGAflux [81ede08e-ab29-11e9-16d3-79edd30a1d76]\n",
      "└ @ Base loading.jl:1273\n",
      "┌ Info: Precompiling Augmentor [02898b10-1f73-11ea-317c-6393d7073e15]\n",
      "└ @ Base loading.jl:1273\n",
      "┌ Info: Precompiling ONNXmutable [cf2a63a0-f8ae-421c-82b7-306ecfceaf66]\n",
      "└ @ Base loading.jl:1273\n",
      "WARNING: Method definition iterate(DataFlow.ObjectArraySet{T} where T, Any...) in module DataFlow at E:\\Programs\\julia\\.julia\\packages\\Lazy\\KYseE\\src\\macros.jl:297 overwritten at E:\\Programs\\julia\\.julia\\packages\\Lazy\\KYseE\\src\\macros.jl:297.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition #iterate(Any, typeof(Base.iterate), DataFlow.ObjectArraySet{T} where T, Any...) in module DataFlow overwritten.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition iterate(DataFlow.ObjectArraySet{T} where T, Any...) in module DataFlow at E:\\Programs\\julia\\.julia\\packages\\Lazy\\KYseE\\src\\macros.jl:297 overwritten at E:\\Programs\\julia\\.julia\\packages\\Lazy\\KYseE\\src\\macros.jl:297.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition #iterate(Any, typeof(Base.iterate), DataFlow.ObjectArraySet{T} where T, Any...) in module DataFlow overwritten.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n"
     ]
    }
   ],
   "source": [
    "include(\"src/Lamarckism.jl\");\n",
    "using Flux\n",
    "using Flux.Optimise\n",
    "using ONNXmutable\n",
    "using Random\n",
    "using Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the backbone model\n",
    "Train the backbone model from the blog post which achieves 91.1% accuracy. This model will be reused in subsequent experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.1,\ttest accuracy: 0.3707\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.5284\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.6477\n",
      "Epoch 4,\tlr: 0.4,\ttest accuracy: 0.5852\n",
      "Epoch 5,\tlr: 0.375,\ttest accuracy: 0.6779\n",
      "Epoch 6,\tlr: 0.35,\ttest accuracy: 0.6825\n",
      "Epoch 7,\tlr: 0.325,\ttest accuracy: 0.7196\n",
      "Epoch 8,\tlr: 0.3,\ttest accuracy: 0.7275\n",
      "Epoch 9,\tlr: 0.275,\ttest accuracy: 0.7527\n",
      "Epoch 10,\tlr: 0.25,\ttest accuracy: 0.7243\n",
      "Epoch 11,\tlr: 0.225,\ttest accuracy: 0.7849\n",
      "Epoch 12,\tlr: 0.2,\ttest accuracy: 0.8187\n",
      "Epoch 13,\tlr: 0.175,\ttest accuracy: 0.8081\n",
      "Epoch 14,\tlr: 0.15,\ttest accuracy: 0.8206\n",
      "Epoch 15,\tlr: 0.125,\ttest accuracy: 0.8155\n",
      "Epoch 16,\tlr: 0.1,\ttest accuracy: 0.8246\n",
      "Epoch 17,\tlr: 0.075,\ttest accuracy: 0.8403\n",
      "Epoch 18,\tlr: 0.05,\ttest accuracy: 0.8689\n",
      "Epoch 19,\tlr: 0.025,\ttest accuracy: 0.8822\n",
      "Epoch 20,\tlr: 0.0,\ttest accuracy: 0.8877\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 20\n",
    "\n",
    "lrs = Lamarckism.lrsched(nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    # Reference experiment does this thing where it divides the learning rate and multiplies the weight decay with batchsize\n",
    "    # These should cancel each other out in the end\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "\n",
    "accuracy_backbone, model_backbone = Lamarckism.train(Lamarckism.backbone(), nepochs, optf);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few % off compared to the reference experiments, but hopefully the difference between models stays the same so comparison is still possible. \n",
    "\n",
    "Save the model so it can be reused without having to retrain it between sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onnx(\"pretrained/backbone.onnx\",model_backbone, (32,32,3,:B));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the resnet model\n",
    "This is the same resnet as used in the blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.1,\ttest accuracy: 0.4701\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.5743\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.6658\n",
      "Epoch 4,\tlr: 0.4,\ttest accuracy: 0.6744\n",
      "Epoch 5,\tlr: 0.375,\ttest accuracy: 0.6691\n",
      "Epoch 6,\tlr: 0.35,\ttest accuracy: 0.7657\n",
      "Epoch 7,\tlr: 0.325,\ttest accuracy: 0.7966\n",
      "Epoch 8,\tlr: 0.3,\ttest accuracy: 0.7854\n",
      "Epoch 9,\tlr: 0.275,\ttest accuracy: 0.8018\n",
      "Epoch 10,\tlr: 0.25,\ttest accuracy: 0.7945\n",
      "Epoch 11,\tlr: 0.225,\ttest accuracy: 0.763\n",
      "Epoch 12,\tlr: 0.2,\ttest accuracy: 0.8265\n",
      "Epoch 13,\tlr: 0.175,\ttest accuracy: 0.8304\n",
      "Epoch 14,\tlr: 0.15,\ttest accuracy: 0.854\n",
      "Epoch 15,\tlr: 0.125,\ttest accuracy: 0.8422\n",
      "Epoch 16,\tlr: 0.1,\ttest accuracy: 0.8619\n",
      "Epoch 17,\tlr: 0.075,\ttest accuracy: 0.8857\n",
      "Epoch 18,\tlr: 0.05,\ttest accuracy: 0.9068\n",
      "Epoch 19,\tlr: 0.025,\ttest accuracy: 0.9152\n",
      "Epoch 20,\tlr: 0.0,\ttest accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 20\n",
    "batchsize = 512\n",
    "\n",
    "lrs = Lamarckism.lrsched(nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    # Reference experiment does this thing where it divides the learning rate and multiplies the weight decay with batchsize\n",
    "    # These should cancel each other out in the end\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "\n",
    "accuracy_resnet, model_resnet = Lamarckism.train(Lamarckism.resnet(), nepochs, optf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this model too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onnx(\"pretrained/resnet.onnx\",model_resnet, (32,32,3,:B));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Impact of identity weight initialization\n",
    "Adding layers to a model typically results in a large degradation in accuracy. This is of course not ideal if one wants to quickly assess if the new model is an improvement over the existing one. \n",
    "\n",
    "One way to prevent this drop is to initialize new layers with an identity mapping. For layers in a residual block this can be achieved by initializing all weights to zero, but this will make the gradient zero as well, preventing any further training of the new weights. \n",
    "\n",
    "Instead, the new layers are initialized with an identity mapping so that they output their input and then scale the output of each resblock with a factor of 0.5. \n",
    "\n",
    "In this experiment, a completely fresh backbone model will be used (i.e not the one trained above). Before any training takes place, two identity mapping residual blocks will be added to it, making its architecture identical to the resnet in the previous example.\n",
    "\n",
    "Question is: Does having parts of the model having a \"suboptimal\" weight initializtion hurt the final performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique.(newpars) = Array{Float32,1}[[0.0, 1.0], [0.0], [1.0], [0.0, 1.0], [0.0], [1.0], [0.0, 1.0], [0.0], [1.0], [0.0, 1.0], [0.0], [1.0]]\n",
      "Epoch 1,\tlr: 0.1,\ttest accuracy: 0.4277\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.6049\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.6713\n",
      "Epoch 4,\tlr: 0.4,\ttest accuracy: 0.6173\n",
      "Epoch 5,\tlr: 0.375,\ttest accuracy: 0.6916\n",
      "Epoch 6,\tlr: 0.35,\ttest accuracy: 0.7204\n",
      "Epoch 7,\tlr: 0.325,\ttest accuracy: 0.7705\n",
      "Epoch 8,\tlr: 0.3,\ttest accuracy: 0.7598\n",
      "Epoch 9,\tlr: 0.275,\ttest accuracy: 0.8285\n",
      "Epoch 10,\tlr: 0.25,\ttest accuracy: 0.7858\n",
      "Epoch 11,\tlr: 0.225,\ttest accuracy: 0.8249\n",
      "Epoch 12,\tlr: 0.2,\ttest accuracy: 0.7584\n",
      "Epoch 13,\tlr: 0.175,\ttest accuracy: 0.835\n",
      "Epoch 14,\tlr: 0.15,\ttest accuracy: 0.8256\n",
      "Epoch 15,\tlr: 0.125,\ttest accuracy: 0.8642\n",
      "Epoch 16,\tlr: 0.1,\ttest accuracy: 0.875\n",
      "Epoch 17,\tlr: 0.075,\ttest accuracy: 0.8759\n",
      "Epoch 18,\tlr: 0.05,\ttest accuracy: 0.9053\n",
      "Epoch 19,\tlr: 0.025,\ttest accuracy: 0.9194\n",
      "Epoch 20,\tlr: 0.0,\ttest accuracy: 0.9214\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 20\n",
    "\n",
    "lrs = Lamarckism.lrsched(nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    # Reference experiment does this thing where it divides the learning rate and multiplies the weight decay with batchsize\n",
    "    # These should cancel each other out in the end\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "\n",
    "model_backbone_add_resblocks, newpars = Lamarckism.addres_fixed(Lamarckism.backbone(), [6,12]);\n",
    "@show unique.(newpars)\n",
    "\n",
    "accuracy_backbone_add_resblocks, model_backbone_add_resblocks = Lamarckism.train(model_backbone_add_resblocks, nepochs, optf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not alot apparently! It seems like having some layers with identity mapping weights does not hurt the final model performance.\n",
    "\n",
    "Lets save this model and plot the accuracy per epoch for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onnx(\"pretrained/backbone_add_resblocks.onnx\",model_backbone_add_resblocks, (32,32,3,:B));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip2000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip2001\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"\n",
       "M211.602 1423.18 L2352.76 1423.18 L2352.76 47.2441 L211.602 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip2002\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"2142\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  697.455,1423.18 697.455,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1229.02,1423.18 1229.02,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1760.59,1423.18 1760.59,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.16,1423.18 2292.16,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.602,1315.18 2352.76,1315.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.602,1079.47 2352.76,1079.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.602,843.756 2352.76,843.756 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.602,608.047 2352.76,608.047 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.602,372.337 2352.76,372.337 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.602,136.628 2352.76,136.628 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.602,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.602,1423.18 211.602,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  697.455,1423.18 697.455,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1229.02,1423.18 1229.02,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1760.59,1423.18 1760.59,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.16,1423.18 2292.16,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.602,1315.18 237.296,1315.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.602,1079.47 237.296,1079.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.602,843.756 237.296,843.756 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.602,608.047 237.296,608.047 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.602,372.337 237.296,372.337 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.602,136.628 237.296,136.628 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"M 0 0 M687.733 1442.09 L706.089 1442.09 L706.089 1446.03 L692.015 1446.03 L692.015 1454.5 Q693.034 1454.15 694.052 1453.99 Q695.071 1453.8 696.089 1453.8 Q701.876 1453.8 705.256 1456.98 Q708.636 1460.15 708.636 1465.56 Q708.636 1471.14 705.163 1474.24 Q701.691 1477.32 695.372 1477.32 Q693.196 1477.32 690.927 1476.95 Q688.682 1476.58 686.275 1475.84 L686.275 1471.14 Q688.358 1472.28 690.58 1472.83 Q692.802 1473.39 695.279 1473.39 Q699.284 1473.39 701.622 1471.28 Q703.96 1469.18 703.96 1465.56 Q703.96 1461.95 701.622 1459.85 Q699.284 1457.74 695.279 1457.74 Q693.404 1457.74 691.529 1458.16 Q689.677 1458.57 687.733 1459.45 L687.733 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1205.9 1472.72 L1213.54 1472.72 L1213.54 1446.35 L1205.23 1448.02 L1205.23 1443.76 L1213.49 1442.09 L1218.17 1442.09 L1218.17 1472.72 L1225.8 1472.72 L1225.8 1476.65 L1205.9 1476.65 L1205.9 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1240.87 1445.17 Q1237.26 1445.17 1235.43 1448.74 Q1233.63 1452.28 1233.63 1459.41 Q1233.63 1466.51 1235.43 1470.08 Q1237.26 1473.62 1240.87 1473.62 Q1244.51 1473.62 1246.31 1470.08 Q1248.14 1466.51 1248.14 1459.41 Q1248.14 1452.28 1246.31 1448.74 Q1244.51 1445.17 1240.87 1445.17 M1240.87 1441.47 Q1246.68 1441.47 1249.74 1446.07 Q1252.82 1450.66 1252.82 1459.41 Q1252.82 1468.13 1249.74 1472.74 Q1246.68 1477.32 1240.87 1477.32 Q1235.06 1477.32 1231.99 1472.74 Q1228.93 1468.13 1228.93 1459.41 Q1228.93 1450.66 1231.99 1446.07 Q1235.06 1441.47 1240.87 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1737.96 1472.72 L1745.6 1472.72 L1745.6 1446.35 L1737.29 1448.02 L1737.29 1443.76 L1745.56 1442.09 L1750.23 1442.09 L1750.23 1472.72 L1757.87 1472.72 L1757.87 1476.65 L1737.96 1476.65 L1737.96 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1762.99 1442.09 L1781.34 1442.09 L1781.34 1446.03 L1767.27 1446.03 L1767.27 1454.5 Q1768.29 1454.15 1769.31 1453.99 Q1770.32 1453.8 1771.34 1453.8 Q1777.13 1453.8 1780.51 1456.98 Q1783.89 1460.15 1783.89 1465.56 Q1783.89 1471.14 1780.42 1474.24 Q1776.94 1477.32 1770.62 1477.32 Q1768.45 1477.32 1766.18 1476.95 Q1763.93 1476.58 1761.53 1475.84 L1761.53 1471.14 Q1763.61 1472.28 1765.83 1472.83 Q1768.06 1473.39 1770.53 1473.39 Q1774.54 1473.39 1776.87 1471.28 Q1779.21 1469.18 1779.21 1465.56 Q1779.21 1461.95 1776.87 1459.85 Q1774.54 1457.74 1770.53 1457.74 Q1768.66 1457.74 1766.78 1458.16 Q1764.93 1458.57 1762.99 1459.45 L1762.99 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2273.3 1472.72 L2289.62 1472.72 L2289.62 1476.65 L2267.68 1476.65 L2267.68 1472.72 Q2270.34 1469.96 2274.92 1465.33 Q2279.53 1460.68 2280.71 1459.34 Q2282.96 1456.81 2283.84 1455.08 Q2284.74 1453.32 2284.74 1451.63 Q2284.74 1448.87 2282.79 1447.14 Q2280.87 1445.4 2277.77 1445.4 Q2275.57 1445.4 2273.12 1446.17 Q2270.69 1446.93 2267.91 1448.48 L2267.91 1443.76 Q2270.73 1442.62 2273.19 1442.05 Q2275.64 1441.47 2277.68 1441.47 Q2283.05 1441.47 2286.24 1444.15 Q2289.44 1446.84 2289.44 1451.33 Q2289.44 1453.46 2288.63 1455.38 Q2287.84 1457.28 2285.73 1459.87 Q2285.15 1460.54 2282.05 1463.76 Q2278.95 1466.95 2273.3 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2304.69 1445.17 Q2301.08 1445.17 2299.25 1448.74 Q2297.45 1452.28 2297.45 1459.41 Q2297.45 1466.51 2299.25 1470.08 Q2301.08 1473.62 2304.69 1473.62 Q2308.33 1473.62 2310.13 1470.08 Q2311.96 1466.51 2311.96 1459.41 Q2311.96 1452.28 2310.13 1448.74 Q2308.33 1445.17 2304.69 1445.17 M2304.69 1441.47 Q2310.5 1441.47 2313.56 1446.07 Q2316.64 1450.66 2316.64 1459.41 Q2316.64 1468.13 2313.56 1472.74 Q2310.5 1477.32 2304.69 1477.32 Q2298.88 1477.32 2295.8 1472.74 Q2292.75 1468.13 2292.75 1459.41 Q2292.75 1450.66 2295.8 1446.07 Q2298.88 1441.47 2304.69 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M138.205 1300.97 Q134.593 1300.97 132.765 1304.54 Q130.959 1308.08 130.959 1315.21 Q130.959 1322.32 132.765 1325.88 Q134.593 1329.42 138.205 1329.42 Q141.839 1329.42 143.644 1325.88 Q145.473 1322.32 145.473 1315.21 Q145.473 1308.08 143.644 1304.54 Q141.839 1300.97 138.205 1300.97 M138.205 1297.27 Q144.015 1297.27 147.07 1301.88 Q150.149 1306.46 150.149 1315.21 Q150.149 1323.94 147.07 1328.54 Q144.015 1333.13 138.205 1333.13 Q132.394 1333.13 129.316 1328.54 Q126.26 1323.94 126.26 1315.21 Q126.26 1306.46 129.316 1301.88 Q132.394 1297.27 138.205 1297.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M155.218 1326.58 L160.103 1326.58 L160.103 1332.46 L155.218 1332.46 L155.218 1326.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M178.019 1301.97 L166.214 1320.42 L178.019 1320.42 L178.019 1301.97 M176.792 1297.9 L182.672 1297.9 L182.672 1320.42 L187.602 1320.42 L187.602 1324.31 L182.672 1324.31 L182.672 1332.46 L178.019 1332.46 L178.019 1324.31 L162.417 1324.31 L162.417 1319.79 L176.792 1297.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M139.686 1065.26 Q136.075 1065.26 134.246 1068.83 Q132.441 1072.37 132.441 1079.5 Q132.441 1086.61 134.246 1090.17 Q136.075 1093.71 139.686 1093.71 Q143.32 1093.71 145.126 1090.17 Q146.954 1086.61 146.954 1079.5 Q146.954 1072.37 145.126 1068.83 Q143.32 1065.26 139.686 1065.26 M139.686 1061.56 Q145.496 1061.56 148.552 1066.17 Q151.63 1070.75 151.63 1079.5 Q151.63 1088.23 148.552 1092.83 Q145.496 1097.42 139.686 1097.42 Q133.876 1097.42 130.797 1092.83 Q127.742 1088.23 127.742 1079.5 Q127.742 1070.75 130.797 1066.17 Q133.876 1061.56 139.686 1061.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M156.7 1090.87 L161.584 1090.87 L161.584 1096.75 L156.7 1096.75 L156.7 1090.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M166.7 1062.19 L185.056 1062.19 L185.056 1066.12 L170.982 1066.12 L170.982 1074.59 Q172.001 1074.25 173.019 1074.08 Q174.038 1073.9 175.056 1073.9 Q180.843 1073.9 184.223 1077.07 Q187.602 1080.24 187.602 1085.66 Q187.602 1091.24 184.13 1094.34 Q180.658 1097.42 174.339 1097.42 Q172.163 1097.42 169.894 1097.05 Q167.649 1096.68 165.241 1095.94 L165.241 1091.24 Q167.325 1092.37 169.547 1092.93 Q171.769 1093.48 174.246 1093.48 Q178.251 1093.48 180.589 1091.38 Q182.927 1089.27 182.927 1085.66 Q182.927 1082.05 180.589 1079.94 Q178.251 1077.83 174.246 1077.83 Q172.371 1077.83 170.496 1078.25 Q168.644 1078.67 166.7 1079.55 L166.7 1062.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M138.529 829.555 Q134.918 829.555 133.089 833.12 Q131.283 836.661 131.283 843.791 Q131.283 850.897 133.089 854.462 Q134.918 858.004 138.529 858.004 Q142.163 858.004 143.968 854.462 Q145.797 850.897 145.797 843.791 Q145.797 836.661 143.968 833.12 Q142.163 829.555 138.529 829.555 M138.529 825.851 Q144.339 825.851 147.394 830.458 Q150.473 835.041 150.473 843.791 Q150.473 852.518 147.394 857.124 Q144.339 861.708 138.529 861.708 Q132.718 861.708 129.64 857.124 Q126.584 852.518 126.584 843.791 Q126.584 835.041 129.64 830.458 Q132.718 825.851 138.529 825.851 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M155.542 855.157 L160.427 855.157 L160.427 861.036 L155.542 861.036 L155.542 855.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M176.075 841.893 Q172.927 841.893 171.075 844.046 Q169.246 846.198 169.246 849.948 Q169.246 853.675 171.075 855.851 Q172.927 858.004 176.075 858.004 Q179.223 858.004 181.052 855.851 Q182.903 853.675 182.903 849.948 Q182.903 846.198 181.052 844.046 Q179.223 841.893 176.075 841.893 M185.357 827.24 L185.357 831.499 Q183.598 830.666 181.792 830.226 Q180.01 829.786 178.251 829.786 Q173.621 829.786 171.167 832.911 Q168.737 836.036 168.39 842.356 Q169.755 840.342 171.815 839.277 Q173.876 838.189 176.352 838.189 Q181.561 838.189 184.57 841.36 Q187.602 844.509 187.602 849.948 Q187.602 855.272 184.454 858.49 Q181.306 861.708 176.075 861.708 Q170.079 861.708 166.908 857.124 Q163.737 852.518 163.737 843.791 Q163.737 835.597 167.626 830.735 Q171.515 825.851 178.065 825.851 Q179.825 825.851 181.607 826.198 Q183.413 826.546 185.357 827.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M139.593 593.845 Q135.982 593.845 134.154 597.41 Q132.348 600.952 132.348 608.081 Q132.348 615.188 134.154 618.753 Q135.982 622.294 139.593 622.294 Q143.228 622.294 145.033 618.753 Q146.862 615.188 146.862 608.081 Q146.862 600.952 145.033 597.41 Q143.228 593.845 139.593 593.845 M139.593 590.142 Q145.404 590.142 148.459 594.748 Q151.538 599.331 151.538 608.081 Q151.538 616.808 148.459 621.415 Q145.404 625.998 139.593 625.998 Q133.783 625.998 130.705 621.415 Q127.649 616.808 127.649 608.081 Q127.649 599.331 130.705 594.748 Q133.783 590.142 139.593 590.142 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M156.607 619.447 L161.491 619.447 L161.491 625.327 L156.607 625.327 L156.607 619.447 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M165.38 590.767 L187.602 590.767 L187.602 592.757 L175.056 625.327 L170.172 625.327 L181.977 594.702 L165.38 594.702 L165.38 590.767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M138.783 358.136 Q135.172 358.136 133.343 361.701 Q131.538 365.242 131.538 372.372 Q131.538 379.478 133.343 383.043 Q135.172 386.585 138.783 386.585 Q142.417 386.585 144.223 383.043 Q146.052 379.478 146.052 372.372 Q146.052 365.242 144.223 361.701 Q142.417 358.136 138.783 358.136 M138.783 354.432 Q144.593 354.432 147.649 359.039 Q150.728 363.622 150.728 372.372 Q150.728 381.099 147.649 385.705 Q144.593 390.288 138.783 390.288 Q132.973 390.288 129.894 385.705 Q126.839 381.099 126.839 372.372 Q126.839 363.622 129.894 359.039 Q132.973 354.432 138.783 354.432 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M155.797 383.737 L160.681 383.737 L160.681 389.617 L155.797 389.617 L155.797 383.737 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M175.751 373.205 Q172.417 373.205 170.496 374.988 Q168.598 376.77 168.598 379.895 Q168.598 383.02 170.496 384.802 Q172.417 386.585 175.751 386.585 Q179.084 386.585 181.005 384.802 Q182.927 382.997 182.927 379.895 Q182.927 376.77 181.005 374.988 Q179.107 373.205 175.751 373.205 M171.075 371.214 Q168.065 370.474 166.376 368.413 Q164.709 366.353 164.709 363.39 Q164.709 359.247 167.649 356.839 Q170.612 354.432 175.751 354.432 Q180.913 354.432 183.852 356.839 Q186.792 359.247 186.792 363.39 Q186.792 366.353 185.102 368.413 Q183.436 370.474 180.45 371.214 Q183.829 372.001 185.704 374.293 Q187.602 376.585 187.602 379.895 Q187.602 384.918 184.524 387.603 Q181.468 390.288 175.751 390.288 Q170.033 390.288 166.954 387.603 Q163.899 384.918 163.899 379.895 Q163.899 376.585 165.797 374.293 Q167.695 372.001 171.075 371.214 M169.362 363.83 Q169.362 366.515 171.028 368.02 Q172.718 369.525 175.751 369.525 Q178.76 369.525 180.45 368.02 Q182.163 366.515 182.163 363.83 Q182.163 361.145 180.45 359.64 Q178.76 358.136 175.751 358.136 Q172.718 358.136 171.028 359.64 Q169.362 361.145 169.362 363.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M138.876 122.426 Q135.265 122.426 133.436 125.991 Q131.63 129.533 131.63 136.662 Q131.63 143.769 133.436 147.333 Q135.265 150.875 138.876 150.875 Q142.51 150.875 144.316 147.333 Q146.144 143.769 146.144 136.662 Q146.144 129.533 144.316 125.991 Q142.51 122.426 138.876 122.426 M138.876 118.723 Q144.686 118.723 147.742 123.329 Q150.82 127.912 150.82 136.662 Q150.82 145.389 147.742 149.996 Q144.686 154.579 138.876 154.579 Q133.066 154.579 129.987 149.996 Q126.931 145.389 126.931 136.662 Q126.931 127.912 129.987 123.329 Q133.066 118.723 138.876 118.723 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M155.89 148.028 L160.774 148.028 L160.774 153.908 L155.89 153.908 L155.89 148.028 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M165.982 153.19 L165.982 148.931 Q167.741 149.764 169.547 150.204 Q171.353 150.644 173.089 150.644 Q177.718 150.644 180.149 147.542 Q182.602 144.417 182.95 138.074 Q181.607 140.065 179.547 141.13 Q177.487 142.195 174.987 142.195 Q169.802 142.195 166.769 139.07 Q163.76 135.922 163.76 130.482 Q163.76 125.158 166.908 121.94 Q170.056 118.723 175.288 118.723 Q181.283 118.723 184.431 123.329 Q187.602 127.912 187.602 136.662 Q187.602 144.833 183.714 149.718 Q179.848 154.579 173.297 154.579 Q171.538 154.579 169.732 154.232 Q167.927 153.884 165.982 153.19 M175.288 138.537 Q178.436 138.537 180.264 136.384 Q182.116 134.232 182.116 130.482 Q182.116 126.755 180.264 124.602 Q178.436 122.426 175.288 122.426 Q172.14 122.426 170.288 124.602 Q168.459 126.755 168.459 130.482 Q168.459 134.232 170.288 136.384 Q172.14 138.537 175.288 138.537 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1196.02 1508.52 L1226.07 1508.52 L1226.07 1513.93 L1202.45 1513.93 L1202.45 1528 L1225.08 1528 L1225.08 1533.41 L1202.45 1533.41 L1202.45 1550.63 L1226.64 1550.63 L1226.64 1556.04 L1196.02 1556.04 L1196.02 1508.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1238.45 1550.7 L1238.45 1569.6 L1232.56 1569.6 L1232.56 1520.4 L1238.45 1520.4 L1238.45 1525.81 Q1240.29 1522.62 1243.09 1521.1 Q1245.93 1519.54 1249.84 1519.54 Q1256.33 1519.54 1260.38 1524.69 Q1264.45 1529.85 1264.45 1538.25 Q1264.45 1546.65 1260.38 1551.81 Q1256.33 1556.97 1249.84 1556.97 Q1245.93 1556.97 1243.09 1555.44 Q1240.29 1553.88 1238.45 1550.7 M1258.37 1538.25 Q1258.37 1531.79 1255.7 1528.13 Q1253.06 1524.44 1248.41 1524.44 Q1243.76 1524.44 1241.09 1528.13 Q1238.45 1531.79 1238.45 1538.25 Q1238.45 1544.71 1241.09 1548.4 Q1243.76 1552.07 1248.41 1552.07 Q1253.06 1552.07 1255.7 1548.4 Q1258.37 1544.71 1258.37 1538.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1284.41 1524.5 Q1279.7 1524.5 1276.96 1528.19 Q1274.22 1531.85 1274.22 1538.25 Q1274.22 1544.65 1276.93 1548.34 Q1279.66 1552 1284.41 1552 Q1289.09 1552 1291.82 1548.31 Q1294.56 1544.62 1294.56 1538.25 Q1294.56 1531.92 1291.82 1528.23 Q1289.09 1524.5 1284.41 1524.5 M1284.41 1519.54 Q1292.05 1519.54 1296.41 1524.5 Q1300.77 1529.47 1300.77 1538.25 Q1300.77 1547 1296.41 1552 Q1292.05 1556.97 1284.41 1556.97 Q1276.74 1556.97 1272.38 1552 Q1268.05 1547 1268.05 1538.25 Q1268.05 1529.47 1272.38 1524.5 Q1276.74 1519.54 1284.41 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1332.56 1521.76 L1332.56 1527.24 Q1330.08 1525.87 1327.57 1525.2 Q1325.08 1524.5 1322.54 1524.5 Q1316.84 1524.5 1313.69 1528.13 Q1310.54 1531.73 1310.54 1538.25 Q1310.54 1544.78 1313.69 1548.4 Q1316.84 1552 1322.54 1552 Q1325.08 1552 1327.57 1551.33 Q1330.08 1550.63 1332.56 1549.26 L1332.56 1554.68 Q1330.11 1555.82 1327.47 1556.39 Q1324.86 1556.97 1321.9 1556.97 Q1313.85 1556.97 1309.11 1551.91 Q1304.36 1546.85 1304.36 1538.25 Q1304.36 1529.53 1309.14 1524.53 Q1313.94 1519.54 1322.28 1519.54 Q1324.99 1519.54 1327.57 1520.11 Q1330.14 1520.65 1332.56 1521.76 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1368.34 1534.53 L1368.34 1556.04 L1362.48 1556.04 L1362.48 1534.72 Q1362.48 1529.66 1360.51 1527.14 Q1358.54 1524.63 1354.59 1524.63 Q1349.85 1524.63 1347.11 1527.65 Q1344.37 1530.68 1344.37 1535.9 L1344.37 1556.04 L1338.48 1556.04 L1338.48 1506.52 L1344.37 1506.52 L1344.37 1525.93 Q1346.47 1522.72 1349.31 1521.13 Q1352.17 1519.54 1355.89 1519.54 Q1362.04 1519.54 1365.19 1523.36 Q1368.34 1527.14 1368.34 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M46.818 848.76 L70.4666 857.481 L70.4666 840.007 L46.818 848.76 M40.4842 852.389 L40.4842 845.1 L88.0042 826.99 L88.0042 833.674 L75.8138 838.002 L75.8138 859.423 L88.0042 863.751 L88.0042 870.531 L40.4842 852.389 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M53.7248 796.339 L59.1993 796.339 Q57.8307 798.821 57.1623 801.336 Q56.4621 803.818 56.4621 806.365 Q56.4621 812.062 60.0905 815.213 Q63.6872 818.364 70.212 818.364 Q76.7369 818.364 80.3653 815.213 Q83.9619 812.062 83.9619 806.365 Q83.9619 803.818 83.2935 801.336 Q82.5933 798.821 81.2247 796.339 L86.6355 796.339 Q87.7814 798.789 88.3543 801.431 Q88.9272 804.041 88.9272 807.001 Q88.9272 815.054 83.8664 819.796 Q78.8057 824.539 70.212 824.539 Q61.491 824.539 56.4939 819.764 Q51.4968 814.958 51.4968 806.619 Q51.4968 803.914 52.0697 801.336 Q52.6108 798.758 53.7248 796.339 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M53.7248 764.542 L59.1993 764.542 Q57.8307 767.025 57.1623 769.539 Q56.4621 772.022 56.4621 774.568 Q56.4621 780.265 60.0905 783.416 Q63.6872 786.567 70.212 786.567 Q76.7369 786.567 80.3653 783.416 Q83.9619 780.265 83.9619 774.568 Q83.9619 772.022 83.2935 769.539 Q82.5933 767.025 81.2247 764.542 L86.6355 764.542 Q87.7814 766.993 88.3543 769.635 Q88.9272 772.244 88.9272 775.205 Q88.9272 783.257 83.8664 788 Q78.8057 792.742 70.212 792.742 Q61.491 792.742 56.4939 787.968 Q51.4968 783.162 51.4968 774.823 Q51.4968 772.117 52.0697 769.539 Q52.6108 766.961 53.7248 764.542 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M73.9359 759.004 L52.3562 759.004 L52.3562 753.147 L73.7131 753.147 Q78.7739 753.147 81.3202 751.174 Q83.8346 749.201 83.8346 745.254 Q83.8346 740.511 80.8109 737.774 Q77.7872 735.005 72.5673 735.005 L52.3562 735.005 L52.3562 729.149 L88.0042 729.149 L88.0042 735.005 L82.5296 735.005 Q85.7762 737.138 87.3676 739.97 Q88.9272 742.771 88.9272 746.495 Q88.9272 752.638 85.1078 755.821 Q81.2883 759.004 73.9359 759.004 M51.4968 744.267 L51.4968 744.267 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M57.8307 702.349 Q57.2578 703.336 57.0032 704.513 Q56.7167 705.659 56.7167 707.06 Q56.7167 712.025 59.9632 714.698 Q63.1779 717.34 69.2253 717.34 L88.0042 717.34 L88.0042 723.229 L52.3562 723.229 L52.3562 717.34 L57.8944 717.34 Q54.6479 715.494 53.0883 712.534 Q51.4968 709.574 51.4968 705.341 Q51.4968 704.736 51.5923 704.004 Q51.656 703.272 51.8151 702.381 L57.8307 702.349 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M70.0847 680.005 Q70.0847 687.103 71.7079 689.84 Q73.3312 692.578 77.2461 692.578 Q80.3653 692.578 82.2114 690.541 Q84.0256 688.472 84.0256 684.939 Q84.0256 680.069 80.5881 677.141 Q77.1188 674.181 71.3897 674.181 L70.0847 674.181 L70.0847 680.005 M67.6657 668.324 L88.0042 668.324 L88.0042 674.181 L82.5933 674.181 Q85.8398 676.186 87.3994 679.178 Q88.9272 682.17 88.9272 686.498 Q88.9272 691.973 85.8716 695.219 Q82.7843 698.434 77.6281 698.434 Q71.6125 698.434 68.5569 694.424 Q65.5014 690.381 65.5014 682.393 L65.5014 674.181 L64.9285 674.181 Q60.8862 674.181 58.6901 676.854 Q56.4621 679.496 56.4621 684.302 Q56.4621 687.358 57.1941 690.254 Q57.9262 693.151 59.3903 695.824 L53.9795 695.824 Q52.7381 692.609 52.1334 689.586 Q51.4968 686.562 51.4968 683.697 Q51.4968 675.963 55.5072 672.144 Q59.5176 668.324 67.6657 668.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M53.7248 636.528 L59.1993 636.528 Q57.8307 639.01 57.1623 641.525 Q56.4621 644.007 56.4621 646.554 Q56.4621 652.251 60.0905 655.402 Q63.6872 658.553 70.212 658.553 Q76.7369 658.553 80.3653 655.402 Q83.9619 652.251 83.9619 646.554 Q83.9619 644.007 83.2935 641.525 Q82.5933 639.01 81.2247 636.528 L86.6355 636.528 Q87.7814 638.978 88.3543 641.62 Q88.9272 644.23 88.9272 647.19 Q88.9272 655.243 83.8664 659.985 Q78.8057 664.728 70.212 664.728 Q61.491 664.728 56.4939 659.953 Q51.4968 655.147 51.4968 646.808 Q51.4968 644.103 52.0697 641.525 Q52.6108 638.947 53.7248 636.528 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M91.3143 615.553 Q97.68 618.035 99.6216 620.391 Q101.563 622.746 101.563 626.693 L101.563 631.371 L96.6615 631.371 L96.6615 627.934 Q96.6615 625.515 95.5157 624.178 Q94.3699 622.841 90.1048 621.218 L87.4312 620.168 L52.3562 634.586 L52.3562 628.379 L80.238 617.24 L52.3562 606.1 L52.3562 599.893 L91.3143 615.553 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip2002)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  272.201,1384.24 378.515,1012.52 484.828,731.323 591.142,878.641 697.455,660.138 803.769,649.296 910.082,561.848 1016.4,543.227 1122.71,483.828 1229.02,550.769 \n",
       "  1335.34,407.929 1441.65,328.259 1547.96,353.245 1654.28,323.781 1760.59,335.802 1866.9,314.353 1973.22,277.346 2079.53,209.933 2185.84,178.584 2292.16,165.62 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  272.201,1149.94 378.515,904.334 484.828,688.659 591.142,668.388 697.455,680.881 803.769,453.185 910.082,380.351 1016.4,406.751 1122.71,368.094 1229.02,385.301 \n",
       "  1335.34,459.55 1441.65,309.874 1547.96,300.681 1654.28,245.054 1760.59,272.868 1866.9,226.433 1973.22,170.334 2079.53,120.599 2185.84,100.8 2292.16,88.3071 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2002)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  272.201,1249.88 378.515,832.206 484.828,675.695 591.142,802.978 697.455,627.846 803.769,559.962 910.082,441.871 1016.4,467.092 1122.71,305.16 1229.02,405.808 \n",
       "  1335.34,313.645 1441.65,470.392 1547.96,289.839 1654.28,311.995 1760.59,221.012 1866.9,195.555 1973.22,193.434 2079.53,124.135 2185.84,90.8999 2292.16,86.1857 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"\n",
       "M1525.09 1279.18 L2280.76 1279.18 L2280.76 1037.26 L1525.09 1037.26  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1525.09,1279.18 2280.76,1279.18 2280.76,1037.26 1525.09,1037.26 1525.09,1279.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip2000)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1549.09,1097.74 1693.09,1097.74 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1721.76 1098.52 L1721.76 1111.18 L1729.26 1111.18 Q1733.04 1111.18 1734.84 1109.63 Q1736.67 1108.05 1736.67 1104.83 Q1736.67 1101.59 1734.84 1100.07 Q1733.04 1098.52 1729.26 1098.52 L1721.76 1098.52 M1721.76 1084.3 L1721.76 1094.72 L1728.68 1094.72 Q1732.11 1094.72 1733.78 1093.45 Q1735.47 1092.15 1735.47 1089.51 Q1735.47 1086.89 1733.78 1085.6 Q1732.11 1084.3 1728.68 1084.3 L1721.76 1084.3 M1717.09 1080.46 L1729.03 1080.46 Q1734.38 1080.46 1737.27 1082.68 Q1740.17 1084.9 1740.17 1089 Q1740.17 1092.17 1738.68 1094.05 Q1737.2 1095.92 1734.33 1096.39 Q1737.78 1097.13 1739.68 1099.49 Q1741.6 1101.83 1741.6 1105.34 Q1741.6 1109.97 1738.45 1112.5 Q1735.3 1115.02 1729.49 1115.02 L1717.09 1115.02 L1717.09 1080.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1757.85 1101.99 Q1752.69 1101.99 1750.7 1103.17 Q1748.71 1104.35 1748.71 1107.2 Q1748.71 1109.46 1750.19 1110.81 Q1751.69 1112.13 1754.26 1112.13 Q1757.8 1112.13 1759.93 1109.63 Q1762.09 1107.1 1762.09 1102.94 L1762.09 1101.99 L1757.85 1101.99 M1766.35 1100.23 L1766.35 1115.02 L1762.09 1115.02 L1762.09 1111.08 Q1760.63 1113.45 1758.45 1114.58 Q1756.28 1115.69 1753.13 1115.69 Q1749.15 1115.69 1746.79 1113.47 Q1744.45 1111.22 1744.45 1107.47 Q1744.45 1103.1 1747.36 1100.88 Q1750.3 1098.65 1756.11 1098.65 L1762.09 1098.65 L1762.09 1098.24 Q1762.09 1095.3 1760.14 1093.7 Q1758.22 1092.08 1754.73 1092.08 Q1752.5 1092.08 1750.4 1092.61 Q1748.29 1093.14 1746.35 1094.21 L1746.35 1090.27 Q1748.68 1089.37 1750.88 1088.93 Q1753.08 1088.47 1755.17 1088.47 Q1760.79 1088.47 1763.57 1091.39 Q1766.35 1094.3 1766.35 1100.23 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1789.47 1090.09 L1789.47 1094.07 Q1787.67 1093.08 1785.84 1092.59 Q1784.03 1092.08 1782.18 1092.08 Q1778.04 1092.08 1775.74 1094.72 Q1773.45 1097.33 1773.45 1102.08 Q1773.45 1106.83 1775.74 1109.46 Q1778.04 1112.08 1782.18 1112.08 Q1784.03 1112.08 1785.84 1111.59 Q1787.67 1111.08 1789.47 1110.09 L1789.47 1114.02 Q1787.69 1114.86 1785.77 1115.27 Q1783.87 1115.69 1781.72 1115.69 Q1775.86 1115.69 1772.41 1112.01 Q1768.96 1108.33 1768.96 1102.08 Q1768.96 1095.74 1772.43 1092.1 Q1775.93 1088.47 1781.99 1088.47 Q1783.96 1088.47 1785.84 1088.89 Q1787.71 1089.28 1789.47 1090.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1793.78 1079 L1798.06 1079 L1798.06 1100.27 L1810.77 1089.09 L1816.21 1089.09 L1802.46 1101.22 L1816.79 1115.02 L1811.23 1115.02 L1798.06 1102.36 L1798.06 1115.02 L1793.78 1115.02 L1793.78 1079 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1839.86 1102.08 Q1839.86 1097.38 1837.92 1094.72 Q1836 1092.03 1832.62 1092.03 Q1829.24 1092.03 1827.3 1094.72 Q1825.37 1097.38 1825.37 1102.08 Q1825.37 1106.78 1827.3 1109.46 Q1829.24 1112.13 1832.62 1112.13 Q1836 1112.13 1837.92 1109.46 Q1839.86 1106.78 1839.86 1102.08 M1825.37 1093.03 Q1826.72 1090.71 1828.75 1089.6 Q1830.81 1088.47 1833.66 1088.47 Q1838.38 1088.47 1841.32 1092.22 Q1844.29 1095.97 1844.29 1102.08 Q1844.29 1108.19 1841.32 1111.94 Q1838.38 1115.69 1833.66 1115.69 Q1830.81 1115.69 1828.75 1114.58 Q1826.72 1113.45 1825.37 1111.13 L1825.37 1115.02 L1821.09 1115.02 L1821.09 1079 L1825.37 1079 L1825.37 1093.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1858.8 1092.08 Q1855.37 1092.08 1853.38 1094.77 Q1851.39 1097.43 1851.39 1102.08 Q1851.39 1106.73 1853.36 1109.42 Q1855.35 1112.08 1858.8 1112.08 Q1862.2 1112.08 1864.19 1109.39 Q1866.18 1106.71 1866.18 1102.08 Q1866.18 1097.47 1864.19 1094.79 Q1862.2 1092.08 1858.8 1092.08 M1858.8 1088.47 Q1864.36 1088.47 1867.53 1092.08 Q1870.7 1095.69 1870.7 1102.08 Q1870.7 1108.45 1867.53 1112.08 Q1864.36 1115.69 1858.8 1115.69 Q1853.22 1115.69 1850.05 1112.08 Q1846.9 1108.45 1846.9 1102.08 Q1846.9 1095.69 1850.05 1092.08 Q1853.22 1088.47 1858.8 1088.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1896.72 1099.37 L1896.72 1115.02 L1892.46 1115.02 L1892.46 1099.51 Q1892.46 1095.83 1891.02 1094 Q1889.59 1092.17 1886.72 1092.17 Q1883.27 1092.17 1881.28 1094.37 Q1879.29 1096.57 1879.29 1100.37 L1879.29 1115.02 L1875 1115.02 L1875 1089.09 L1879.29 1089.09 L1879.29 1093.12 Q1880.81 1090.78 1882.87 1089.63 Q1884.96 1088.47 1887.67 1088.47 Q1892.13 1088.47 1894.42 1091.25 Q1896.72 1094 1896.72 1099.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1923.36 1100.99 L1923.36 1103.08 L1903.78 1103.08 Q1904.05 1107.47 1906.42 1109.79 Q1908.8 1112.08 1913.04 1112.08 Q1915.49 1112.08 1917.78 1111.48 Q1920.1 1110.88 1922.36 1109.67 L1922.36 1113.7 Q1920.07 1114.67 1917.67 1115.18 Q1915.26 1115.69 1912.78 1115.69 Q1906.58 1115.69 1902.94 1112.08 Q1899.33 1108.47 1899.33 1102.31 Q1899.33 1095.95 1902.76 1092.22 Q1906.21 1088.47 1912.04 1088.47 Q1917.27 1088.47 1920.3 1091.85 Q1923.36 1095.21 1923.36 1100.99 M1919.1 1099.74 Q1919.05 1096.25 1917.13 1094.16 Q1915.23 1092.08 1912.09 1092.08 Q1908.52 1092.08 1906.37 1094.09 Q1904.24 1096.11 1903.92 1099.77 L1919.1 1099.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip2000)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1549.09,1158.22 1693.09,1158.22 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1733.48 1159.3 Q1734.98 1159.81 1736.39 1161.47 Q1737.83 1163.14 1739.26 1166.06 L1744.01 1175.5 L1738.99 1175.5 L1734.56 1166.63 Q1732.85 1163.16 1731.23 1162.03 Q1729.63 1160.89 1726.86 1160.89 L1721.76 1160.89 L1721.76 1175.5 L1717.09 1175.5 L1717.09 1140.94 L1727.64 1140.94 Q1733.57 1140.94 1736.49 1143.42 Q1739.4 1145.89 1739.4 1150.89 Q1739.4 1154.16 1737.87 1156.31 Q1736.37 1158.46 1733.48 1159.3 M1721.76 1144.78 L1721.76 1157.05 L1727.64 1157.05 Q1731.02 1157.05 1732.74 1155.5 Q1734.47 1153.93 1734.47 1150.89 Q1734.47 1147.86 1732.74 1146.33 Q1731.02 1144.78 1727.64 1144.78 L1721.76 1144.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1768.52 1161.47 L1768.52 1163.56 L1748.94 1163.56 Q1749.22 1167.95 1751.58 1170.27 Q1753.96 1172.56 1758.2 1172.56 Q1760.65 1172.56 1762.94 1171.96 Q1765.26 1171.36 1767.53 1170.15 L1767.53 1174.18 Q1765.24 1175.15 1762.83 1175.66 Q1760.42 1176.17 1757.94 1176.17 Q1751.74 1176.17 1748.11 1172.56 Q1744.49 1168.95 1744.49 1162.79 Q1744.49 1156.43 1747.92 1152.7 Q1751.37 1148.95 1757.2 1148.95 Q1762.43 1148.95 1765.47 1152.33 Q1768.52 1155.69 1768.52 1161.47 M1764.26 1160.22 Q1764.22 1156.73 1762.3 1154.64 Q1760.4 1152.56 1757.25 1152.56 Q1753.68 1152.56 1751.53 1154.57 Q1749.4 1156.59 1749.08 1160.25 L1764.26 1160.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1789.52 1150.34 L1789.52 1154.37 Q1787.71 1153.44 1785.77 1152.98 Q1783.82 1152.51 1781.74 1152.51 Q1778.57 1152.51 1776.97 1153.49 Q1775.4 1154.46 1775.4 1156.4 Q1775.4 1157.88 1776.53 1158.74 Q1777.67 1159.57 1781.09 1160.34 L1782.55 1160.66 Q1787.09 1161.63 1788.99 1163.42 Q1790.91 1165.18 1790.91 1168.35 Q1790.91 1171.96 1788.04 1174.06 Q1785.19 1176.17 1780.19 1176.17 Q1778.11 1176.17 1775.84 1175.75 Q1773.59 1175.36 1771.09 1174.55 L1771.09 1170.15 Q1773.45 1171.38 1775.74 1172 Q1778.04 1172.61 1780.28 1172.61 Q1783.29 1172.61 1784.91 1171.59 Q1786.53 1170.55 1786.53 1168.67 Q1786.53 1166.94 1785.35 1166.01 Q1784.19 1165.08 1780.24 1164.23 L1778.75 1163.88 Q1774.8 1163.05 1773.04 1161.33 Q1771.28 1159.6 1771.28 1156.59 Q1771.28 1152.93 1773.87 1150.94 Q1776.46 1148.95 1781.23 1148.95 Q1783.59 1148.95 1785.67 1149.3 Q1787.76 1149.64 1789.52 1150.34 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1816.92 1159.85 L1816.92 1175.5 L1812.67 1175.5 L1812.67 1159.99 Q1812.67 1156.31 1811.23 1154.48 Q1809.8 1152.65 1806.92 1152.65 Q1803.48 1152.65 1801.49 1154.85 Q1799.49 1157.05 1799.49 1160.85 L1799.49 1175.5 L1795.21 1175.5 L1795.21 1149.57 L1799.49 1149.57 L1799.49 1153.6 Q1801.02 1151.26 1803.08 1150.11 Q1805.17 1148.95 1807.87 1148.95 Q1812.34 1148.95 1814.63 1151.73 Q1816.92 1154.48 1816.92 1159.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1843.57 1161.47 L1843.57 1163.56 L1823.98 1163.56 Q1824.26 1167.95 1826.62 1170.27 Q1829.01 1172.56 1833.24 1172.56 Q1835.7 1172.56 1837.99 1171.96 Q1840.3 1171.36 1842.57 1170.15 L1842.57 1174.18 Q1840.28 1175.15 1837.87 1175.66 Q1835.47 1176.17 1832.99 1176.17 Q1826.79 1176.17 1823.15 1172.56 Q1819.54 1168.95 1819.54 1162.79 Q1819.54 1156.43 1822.97 1152.7 Q1826.42 1148.95 1832.25 1148.95 Q1837.48 1148.95 1840.51 1152.33 Q1843.57 1155.69 1843.57 1161.47 M1839.31 1160.22 Q1839.26 1156.73 1837.34 1154.64 Q1835.44 1152.56 1832.3 1152.56 Q1828.73 1152.56 1826.58 1154.57 Q1824.45 1156.59 1824.12 1160.25 L1839.31 1160.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1852.25 1142.21 L1852.25 1149.57 L1861.02 1149.57 L1861.02 1152.88 L1852.25 1152.88 L1852.25 1166.96 Q1852.25 1170.13 1853.11 1171.03 Q1853.98 1171.94 1856.65 1171.94 L1861.02 1171.94 L1861.02 1175.5 L1856.65 1175.5 Q1851.72 1175.5 1849.84 1173.67 Q1847.97 1171.82 1847.97 1166.96 L1847.97 1152.88 L1844.84 1152.88 L1844.84 1149.57 L1847.97 1149.57 L1847.97 1142.21 L1852.25 1142.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip2000)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1549.09,1218.7 1693.09,1218.7 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1721.76 1219.48 L1721.76 1232.14 L1729.26 1232.14 Q1733.04 1232.14 1734.84 1230.59 Q1736.67 1229.01 1736.67 1225.79 Q1736.67 1222.55 1734.84 1221.03 Q1733.04 1219.48 1729.26 1219.48 L1721.76 1219.48 M1721.76 1205.26 L1721.76 1215.68 L1728.68 1215.68 Q1732.11 1215.68 1733.78 1214.41 Q1735.47 1213.11 1735.47 1210.47 Q1735.47 1207.85 1733.78 1206.56 Q1732.11 1205.26 1728.68 1205.26 L1721.76 1205.26 M1717.09 1201.42 L1729.03 1201.42 Q1734.38 1201.42 1737.27 1203.64 Q1740.17 1205.86 1740.17 1209.96 Q1740.17 1213.13 1738.68 1215.01 Q1737.2 1216.88 1734.33 1217.35 Q1737.78 1218.09 1739.68 1220.45 Q1741.6 1222.79 1741.6 1226.3 Q1741.6 1230.93 1738.45 1233.46 Q1735.3 1235.98 1729.49 1235.98 L1717.09 1235.98 L1717.09 1201.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1757.85 1222.95 Q1752.69 1222.95 1750.7 1224.13 Q1748.71 1225.31 1748.71 1228.16 Q1748.71 1230.42 1750.19 1231.77 Q1751.69 1233.09 1754.26 1233.09 Q1757.8 1233.09 1759.93 1230.59 Q1762.09 1228.06 1762.09 1223.9 L1762.09 1222.95 L1757.85 1222.95 M1766.35 1221.19 L1766.35 1235.98 L1762.09 1235.98 L1762.09 1232.04 Q1760.63 1234.41 1758.45 1235.54 Q1756.28 1236.65 1753.13 1236.65 Q1749.15 1236.65 1746.79 1234.43 Q1744.45 1232.18 1744.45 1228.43 Q1744.45 1224.06 1747.36 1221.84 Q1750.3 1219.61 1756.11 1219.61 L1762.09 1219.61 L1762.09 1219.2 Q1762.09 1216.26 1760.14 1214.66 Q1758.22 1213.04 1754.73 1213.04 Q1752.5 1213.04 1750.4 1213.57 Q1748.29 1214.1 1746.35 1215.17 L1746.35 1211.23 Q1748.68 1210.33 1750.88 1209.89 Q1753.08 1209.43 1755.17 1209.43 Q1760.79 1209.43 1763.57 1212.35 Q1766.35 1215.26 1766.35 1221.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1789.47 1211.05 L1789.47 1215.03 Q1787.67 1214.04 1785.84 1213.55 Q1784.03 1213.04 1782.18 1213.04 Q1778.04 1213.04 1775.74 1215.68 Q1773.45 1218.29 1773.45 1223.04 Q1773.45 1227.79 1775.74 1230.42 Q1778.04 1233.04 1782.18 1233.04 Q1784.03 1233.04 1785.84 1232.55 Q1787.67 1232.04 1789.47 1231.05 L1789.47 1234.98 Q1787.69 1235.82 1785.77 1236.23 Q1783.87 1236.65 1781.72 1236.65 Q1775.86 1236.65 1772.41 1232.97 Q1768.96 1229.29 1768.96 1223.04 Q1768.96 1216.7 1772.43 1213.06 Q1775.93 1209.43 1781.99 1209.43 Q1783.96 1209.43 1785.84 1209.85 Q1787.71 1210.24 1789.47 1211.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1793.78 1199.96 L1798.06 1199.96 L1798.06 1221.23 L1810.77 1210.05 L1816.21 1210.05 L1802.46 1222.18 L1816.79 1235.98 L1811.23 1235.98 L1798.06 1223.32 L1798.06 1235.98 L1793.78 1235.98 L1793.78 1199.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1839.86 1223.04 Q1839.86 1218.34 1837.92 1215.68 Q1836 1212.99 1832.62 1212.99 Q1829.24 1212.99 1827.3 1215.68 Q1825.37 1218.34 1825.37 1223.04 Q1825.37 1227.74 1827.3 1230.42 Q1829.24 1233.09 1832.62 1233.09 Q1836 1233.09 1837.92 1230.42 Q1839.86 1227.74 1839.86 1223.04 M1825.37 1213.99 Q1826.72 1211.67 1828.75 1210.56 Q1830.81 1209.43 1833.66 1209.43 Q1838.38 1209.43 1841.32 1213.18 Q1844.29 1216.93 1844.29 1223.04 Q1844.29 1229.15 1841.32 1232.9 Q1838.38 1236.65 1833.66 1236.65 Q1830.81 1236.65 1828.75 1235.54 Q1826.72 1234.41 1825.37 1232.09 L1825.37 1235.98 L1821.09 1235.98 L1821.09 1199.96 L1825.37 1199.96 L1825.37 1213.99 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1858.8 1213.04 Q1855.37 1213.04 1853.38 1215.73 Q1851.39 1218.39 1851.39 1223.04 Q1851.39 1227.69 1853.36 1230.38 Q1855.35 1233.04 1858.8 1233.04 Q1862.2 1233.04 1864.19 1230.35 Q1866.18 1227.67 1866.18 1223.04 Q1866.18 1218.43 1864.19 1215.75 Q1862.2 1213.04 1858.8 1213.04 M1858.8 1209.43 Q1864.36 1209.43 1867.53 1213.04 Q1870.7 1216.65 1870.7 1223.04 Q1870.7 1229.41 1867.53 1233.04 Q1864.36 1236.65 1858.8 1236.65 Q1853.22 1236.65 1850.05 1233.04 Q1846.9 1229.41 1846.9 1223.04 Q1846.9 1216.65 1850.05 1213.04 Q1853.22 1209.43 1858.8 1209.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1896.72 1220.33 L1896.72 1235.98 L1892.46 1235.98 L1892.46 1220.47 Q1892.46 1216.79 1891.02 1214.96 Q1889.59 1213.13 1886.72 1213.13 Q1883.27 1213.13 1881.28 1215.33 Q1879.29 1217.53 1879.29 1221.33 L1879.29 1235.98 L1875 1235.98 L1875 1210.05 L1879.29 1210.05 L1879.29 1214.08 Q1880.81 1211.74 1882.87 1210.59 Q1884.96 1209.43 1887.67 1209.43 Q1892.13 1209.43 1894.42 1212.21 Q1896.72 1214.96 1896.72 1220.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1923.36 1221.95 L1923.36 1224.04 L1903.78 1224.04 Q1904.05 1228.43 1906.42 1230.75 Q1908.8 1233.04 1913.04 1233.04 Q1915.49 1233.04 1917.78 1232.44 Q1920.1 1231.84 1922.36 1230.63 L1922.36 1234.66 Q1920.07 1235.63 1917.67 1236.14 Q1915.26 1236.65 1912.78 1236.65 Q1906.58 1236.65 1902.94 1233.04 Q1899.33 1229.43 1899.33 1223.27 Q1899.33 1216.91 1902.76 1213.18 Q1906.21 1209.43 1912.04 1209.43 Q1917.27 1209.43 1920.3 1212.81 Q1923.36 1216.17 1923.36 1221.95 M1919.1 1220.7 Q1919.05 1217.21 1917.13 1215.12 Q1915.23 1213.04 1912.09 1213.04 Q1908.52 1213.04 1906.37 1215.05 Q1904.24 1217.07 1903.92 1220.73 L1919.1 1220.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1954.68 1222.95 Q1949.52 1222.95 1947.53 1224.13 Q1945.54 1225.31 1945.54 1228.16 Q1945.54 1230.42 1947.02 1231.77 Q1948.52 1233.09 1951.09 1233.09 Q1954.63 1233.09 1956.76 1230.59 Q1958.91 1228.06 1958.91 1223.9 L1958.91 1222.95 L1954.68 1222.95 M1963.17 1221.19 L1963.17 1235.98 L1958.91 1235.98 L1958.91 1232.04 Q1957.46 1234.41 1955.28 1235.54 Q1953.1 1236.65 1949.96 1236.65 Q1945.98 1236.65 1943.61 1234.43 Q1941.28 1232.18 1941.28 1228.43 Q1941.28 1224.06 1944.19 1221.84 Q1947.13 1219.61 1952.94 1219.61 L1958.91 1219.61 L1958.91 1219.2 Q1958.91 1216.26 1956.97 1214.66 Q1955.05 1213.04 1951.55 1213.04 Q1949.33 1213.04 1947.23 1213.57 Q1945.12 1214.1 1943.17 1215.17 L1943.17 1211.23 Q1945.51 1210.33 1947.71 1209.89 Q1949.91 1209.43 1951.99 1209.43 Q1957.62 1209.43 1960.4 1212.35 Q1963.17 1215.26 1963.17 1221.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M1984.7 1213.99 L1984.7 1199.96 L1988.96 1199.96 L1988.96 1235.98 L1984.7 1235.98 L1984.7 1232.09 Q1983.36 1234.41 1981.3 1235.54 Q1979.26 1236.65 1976.39 1236.65 Q1971.69 1236.65 1968.73 1232.9 Q1965.79 1229.15 1965.79 1223.04 Q1965.79 1216.93 1968.73 1213.18 Q1971.69 1209.43 1976.39 1209.43 Q1979.26 1209.43 1981.3 1210.56 Q1983.36 1211.67 1984.7 1213.99 M1970.19 1223.04 Q1970.19 1227.74 1972.11 1230.42 Q1974.05 1233.09 1977.43 1233.09 Q1980.81 1233.09 1982.76 1230.42 Q1984.7 1227.74 1984.7 1223.04 Q1984.7 1218.34 1982.76 1215.68 Q1980.81 1212.99 1977.43 1212.99 Q1974.05 1212.99 1972.11 1215.68 Q1970.19 1218.34 1970.19 1223.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2010.49 1213.99 L2010.49 1199.96 L2014.75 1199.96 L2014.75 1235.98 L2010.49 1235.98 L2010.49 1232.09 Q2009.15 1234.41 2007.09 1235.54 Q2005.05 1236.65 2002.18 1236.65 Q1997.48 1236.65 1994.52 1232.9 Q1991.58 1229.15 1991.58 1223.04 Q1991.58 1216.93 1994.52 1213.18 Q1997.48 1209.43 2002.18 1209.43 Q2005.05 1209.43 2007.09 1210.56 Q2009.15 1211.67 2010.49 1213.99 M1995.97 1223.04 Q1995.97 1227.74 1997.9 1230.42 Q1999.84 1233.09 2003.22 1233.09 Q2006.6 1233.09 2008.54 1230.42 Q2010.49 1227.74 2010.49 1223.04 Q2010.49 1218.34 2008.54 1215.68 Q2006.6 1212.99 2003.22 1212.99 Q1999.84 1212.99 1997.9 1215.68 Q1995.97 1218.34 1995.97 1223.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2049.31 1214.04 Q2048.59 1213.62 2047.73 1213.43 Q2046.9 1213.23 2045.88 1213.23 Q2042.27 1213.23 2040.33 1215.59 Q2038.41 1217.92 2038.41 1222.32 L2038.41 1235.98 L2034.12 1235.98 L2034.12 1210.05 L2038.41 1210.05 L2038.41 1214.08 Q2039.75 1211.72 2041.9 1210.59 Q2044.05 1209.43 2047.13 1209.43 Q2047.57 1209.43 2048.1 1209.5 Q2048.64 1209.54 2049.28 1209.66 L2049.31 1214.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2074.91 1221.95 L2074.91 1224.04 L2055.33 1224.04 Q2055.6 1228.43 2057.97 1230.75 Q2060.35 1233.04 2064.59 1233.04 Q2067.04 1233.04 2069.33 1232.44 Q2071.65 1231.84 2073.91 1230.63 L2073.91 1234.66 Q2071.62 1235.63 2069.22 1236.14 Q2066.81 1236.65 2064.33 1236.65 Q2058.13 1236.65 2054.49 1233.04 Q2050.88 1229.43 2050.88 1223.27 Q2050.88 1216.91 2054.31 1213.18 Q2057.76 1209.43 2063.59 1209.43 Q2068.82 1209.43 2071.85 1212.81 Q2074.91 1216.17 2074.91 1221.95 M2070.65 1220.7 Q2070.6 1217.21 2068.68 1215.12 Q2066.78 1213.04 2063.64 1213.04 Q2060.07 1213.04 2057.92 1215.05 Q2055.79 1217.07 2055.47 1220.73 L2070.65 1220.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2095.9 1210.82 L2095.9 1214.85 Q2094.1 1213.92 2092.15 1213.46 Q2090.21 1212.99 2088.13 1212.99 Q2084.96 1212.99 2083.36 1213.97 Q2081.78 1214.94 2081.78 1216.88 Q2081.78 1218.36 2082.92 1219.22 Q2084.05 1220.05 2087.48 1220.82 L2088.94 1221.14 Q2093.47 1222.11 2095.37 1223.9 Q2097.29 1225.66 2097.29 1228.83 Q2097.29 1232.44 2094.42 1234.54 Q2091.58 1236.65 2086.58 1236.65 Q2084.49 1236.65 2082.22 1236.23 Q2079.98 1235.84 2077.48 1235.03 L2077.48 1230.63 Q2079.84 1231.86 2082.13 1232.48 Q2084.42 1233.09 2086.67 1233.09 Q2089.68 1233.09 2091.3 1232.07 Q2092.92 1231.03 2092.92 1229.15 Q2092.92 1227.42 2091.74 1226.49 Q2090.58 1225.56 2086.62 1224.71 L2085.14 1224.36 Q2081.18 1223.53 2079.42 1221.81 Q2077.66 1220.08 2077.66 1217.07 Q2077.66 1213.41 2080.26 1211.42 Q2082.85 1209.43 2087.62 1209.43 Q2089.98 1209.43 2092.06 1209.78 Q2094.15 1210.12 2095.9 1210.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2120.37 1223.04 Q2120.37 1218.34 2118.43 1215.68 Q2116.51 1212.99 2113.13 1212.99 Q2109.75 1212.99 2107.8 1215.68 Q2105.88 1218.34 2105.88 1223.04 Q2105.88 1227.74 2107.8 1230.42 Q2109.75 1233.09 2113.13 1233.09 Q2116.51 1233.09 2118.43 1230.42 Q2120.37 1227.74 2120.37 1223.04 M2105.88 1213.99 Q2107.22 1211.67 2109.26 1210.56 Q2111.32 1209.43 2114.17 1209.43 Q2118.89 1209.43 2121.83 1213.18 Q2124.79 1216.93 2124.79 1223.04 Q2124.79 1229.15 2121.83 1232.9 Q2118.89 1236.65 2114.17 1236.65 Q2111.32 1236.65 2109.26 1235.54 Q2107.22 1234.41 2105.88 1232.09 L2105.88 1235.98 L2101.6 1235.98 L2101.6 1199.96 L2105.88 1199.96 L2105.88 1213.99 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2129.26 1199.96 L2133.52 1199.96 L2133.52 1235.98 L2129.26 1235.98 L2129.26 1199.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2148.03 1213.04 Q2144.61 1213.04 2142.62 1215.73 Q2140.63 1218.39 2140.63 1223.04 Q2140.63 1227.69 2142.59 1230.38 Q2144.59 1233.04 2148.03 1233.04 Q2151.44 1233.04 2153.43 1230.35 Q2155.42 1227.67 2155.42 1223.04 Q2155.42 1218.43 2153.43 1215.75 Q2151.44 1213.04 2148.03 1213.04 M2148.03 1209.43 Q2153.59 1209.43 2156.76 1213.04 Q2159.93 1216.65 2159.93 1223.04 Q2159.93 1229.41 2156.76 1233.04 Q2153.59 1236.65 2148.03 1236.65 Q2142.46 1236.65 2139.28 1233.04 Q2136.14 1229.41 2136.14 1223.04 Q2136.14 1216.65 2139.28 1213.04 Q2142.46 1209.43 2148.03 1209.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2183.06 1211.05 L2183.06 1215.03 Q2181.25 1214.04 2179.42 1213.55 Q2177.62 1213.04 2175.77 1213.04 Q2171.62 1213.04 2169.33 1215.68 Q2167.04 1218.29 2167.04 1223.04 Q2167.04 1227.79 2169.33 1230.42 Q2171.62 1233.04 2175.77 1233.04 Q2177.62 1233.04 2179.42 1232.55 Q2181.25 1232.04 2183.06 1231.05 L2183.06 1234.98 Q2181.27 1235.82 2179.35 1236.23 Q2177.46 1236.65 2175.3 1236.65 Q2169.45 1236.65 2166 1232.97 Q2162.55 1229.29 2162.55 1223.04 Q2162.55 1216.7 2166.02 1213.06 Q2169.52 1209.43 2175.58 1209.43 Q2177.55 1209.43 2179.42 1209.85 Q2181.3 1210.24 2183.06 1211.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2187.36 1199.96 L2191.65 1199.96 L2191.65 1221.23 L2204.35 1210.05 L2209.79 1210.05 L2196.04 1222.18 L2210.37 1235.98 L2204.82 1235.98 L2191.65 1223.32 L2191.65 1235.98 L2187.36 1235.98 L2187.36 1199.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip2000)\" d=\"M 0 0 M2231.37 1210.82 L2231.37 1214.85 Q2229.56 1213.92 2227.62 1213.46 Q2225.67 1212.99 2223.59 1212.99 Q2220.42 1212.99 2218.82 1213.97 Q2217.25 1214.94 2217.25 1216.88 Q2217.25 1218.36 2218.38 1219.22 Q2219.52 1220.05 2222.94 1220.82 L2224.4 1221.14 Q2228.94 1222.11 2230.83 1223.9 Q2232.76 1225.66 2232.76 1228.83 Q2232.76 1232.44 2229.89 1234.54 Q2227.04 1236.65 2222.04 1236.65 Q2219.96 1236.65 2217.69 1236.23 Q2215.44 1235.84 2212.94 1235.03 L2212.94 1230.63 Q2215.3 1231.86 2217.59 1232.48 Q2219.89 1233.09 2222.13 1233.09 Q2225.14 1233.09 2226.76 1232.07 Q2228.38 1231.03 2228.38 1229.15 Q2228.38 1227.42 2227.2 1226.49 Q2226.04 1225.56 2222.08 1224.71 L2220.6 1224.36 Q2216.64 1223.53 2214.89 1221.81 Q2213.13 1220.08 2213.13 1217.07 Q2213.13 1213.41 2215.72 1211.42 Q2218.31 1209.43 2223.08 1209.43 Q2225.44 1209.43 2227.52 1209.78 Q2229.61 1210.12 2231.37 1210.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(accuracy_backbone, label=\"Backbone\");\n",
    "plot!(accuracy_resnet, label=\"Resnet\");\n",
    "plot!(accuracy_backbone_add_resblocks, label=\"Backbone add resblocks\", xlabel=\"Epoch\", ylabel=\"Accuracy\", legend=:bottomright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View models using [netron](https://lutzroeder.github.io/netron):<br>\n",
    "[backbone](https://lutzroeder.github.io/netron?url=https://github.com/DrChainsaw/NaiveGAExperiments/blob/master/lamarckism/pretrained/backbone.onnx)<br>\n",
    "[resnet](https://lutzroeder.github.io/netron?url=https://github.com/DrChainsaw/NaiveGAExperiments/blob/master/lamarckism/pretrained/resnet.onnx)<br>\n",
    "[backbone with added resblocks](https://lutzroeder.github.io/netron?url=https://github.com/DrChainsaw/NaiveGAExperiments/blob/master/lamarckism/pretrained/backbone_add_resblocks.onnx)\n",
    "\n",
    "## Experiment 2: Fully retrain a pretrained model\n",
    "\n",
    "Now lets see what happens if the already trained backbone model is transformed to a resnet and then re-trained using the same number of epochs and learning rate schedule as the resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique.(newpars) = Array{Float32,1}[[0.0, 1.0], [0.0], [1.0], [0.0, 1.0], [0.0], [1.0], [0.0, 1.0], [0.0], [1.0], [0.0, 1.0], [0.0], [1.0]]\n",
      "Epoch 1,\tlr: 0.1,\ttest accuracy: 0.823\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.7854\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.7742\n",
      "Epoch 4,\tlr: 0.4,\ttest accuracy: 0.6869\n",
      "Epoch 5,\tlr: 0.375,\ttest accuracy: 0.7344\n",
      "Epoch 6,\tlr: 0.35,\ttest accuracy: 0.7412\n",
      "Epoch 7,\tlr: 0.325,\ttest accuracy: 0.7471\n",
      "Epoch 8,\tlr: 0.3,\ttest accuracy: 0.7949\n",
      "Epoch 9,\tlr: 0.275,\ttest accuracy: 0.8357\n",
      "Epoch 10,\tlr: 0.25,\ttest accuracy: 0.8496\n",
      "Epoch 11,\tlr: 0.225,\ttest accuracy: 0.8391\n",
      "Epoch 12,\tlr: 0.2,\ttest accuracy: 0.8689\n",
      "Epoch 13,\tlr: 0.175,\ttest accuracy: 0.8323\n",
      "Epoch 14,\tlr: 0.15,\ttest accuracy: 0.8487\n",
      "Epoch 15,\tlr: 0.125,\ttest accuracy: 0.8785\n",
      "Epoch 16,\tlr: 0.1,\ttest accuracy: 0.878\n",
      "Epoch 17,\tlr: 0.075,\ttest accuracy: 0.8904\n",
      "Epoch 18,\tlr: 0.05,\ttest accuracy: 0.9192\n",
      "Epoch 19,\tlr: 0.025,\ttest accuracy: 0.9281\n",
      "Epoch 20,\tlr: 0.0,\ttest accuracy: 0.9321\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 20\n",
    "\n",
    "lrs = Lamarckism.lrsched(nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "    \n",
    "model, newpars = Lamarckism.addres_fixed(Lamarckism.pretrained(\"pretrained/backbone.onnx\"), [6,12]);\n",
    "@show unique.(newpars)\n",
    "    \n",
    "Lamarckism.train(model, nepochs, optf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to reach the same performance as the resnet which is good. The transformed model starts out having much better accuracy than a randomly initialized model, but it takes a substantial hit in performance after the first epoch and after a few epochs the accuracy basically follows the same evolution as when a randomly initialized resnet is trained. \n",
    "\n",
    "Is it possible to do better?\n",
    "\n",
    "## Experiment 3: Fine tune a pretrained model\n",
    "In this experiment the pretrained backbone is again transformed to the resnet. As the goal is to fine tune the new layers one valid assumption is to start off with a lower learning rate.\n",
    "\n",
    "As seen in experiment 2, a learning rate of 0.1 was detrimental to the performance. One can also see that after the learning rate hits 0.075 the accuracy starts to improve beyond what the backbone model achieved.\n",
    "\n",
    "As such, lets try to fine tune the pretrained model for a generous 5 epochs starting with a learning rate of 0.05, linearly decreasing it to 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.05,\ttest accuracy: 0.8539\n",
      "Epoch 2,\tlr: 0.0375,\ttest accuracy: 0.8626\n",
      "Epoch 3,\tlr: 0.025,\ttest accuracy: 0.8842\n",
      "Epoch 4,\tlr: 0.0125,\ttest accuracy: 0.8919\n",
      "Epoch 5,\tlr: 0.0,\ttest accuracy: 0.8951\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 5\n",
    "\n",
    "lrs = range(0.05,0.0, length=nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "    \n",
    "model, newpars = Lamarckism.addres_fixed(Lamarckism.pretrained(\"pretrained/backbone.onnx\"), [6,12]);\n",
    "    \n",
    "Lamarckism.train(model, nepochs, optf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drop in accuracy is not as severe, but it is still significant. Also, it did also not reach the same accuracy in the end.\n",
    "\n",
    "Lets apply the same principle again: Model accuracy starts beating the baseline at learning rate of 0.025. Lets try to start from there. Also do more epochs to see if it is possible to reach same accuracy if the model is trained longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.025,\ttest accuracy: 0.8754\n",
      "Epoch 2,\tlr: 0.0222,\ttest accuracy: 0.8723\n",
      "Epoch 3,\tlr: 0.0194,\ttest accuracy: 0.889\n",
      "Epoch 4,\tlr: 0.0167,\ttest accuracy: 0.8873\n",
      "Epoch 5,\tlr: 0.0139,\ttest accuracy: 0.8914\n",
      "Epoch 6,\tlr: 0.0111,\ttest accuracy: 0.8897\n",
      "Epoch 7,\tlr: 0.00833,\ttest accuracy: 0.8986\n",
      "Epoch 8,\tlr: 0.00556,\ttest accuracy: 0.9002\n",
      "Epoch 9,\tlr: 0.00278,\ttest accuracy: 0.9005\n",
      "Epoch 10,\tlr: 0.0,\ttest accuracy: 0.9005\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 10\n",
    "\n",
    "lrs = range(0.025,0.0, length=nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "    \n",
    "model, newpars = Lamarckism.addres_fixed(Lamarckism.pretrained(\"pretrained/backbone.onnx\"), [6,12]);\n",
    "    \n",
    "Lamarckism.train(model, nepochs, optf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! The model accuracy did not drop significantly but the training was slow and in the end we could not match the accuracy of the scratch trained resnet. \n",
    "\n",
    "Perhaps the reason was the extremely low learning rates for most of the training which without any built up momentum were just not sufficient to push the model to a better spot.\n",
    "\n",
    "Lets try to keep the learning rate constant and see where we end up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.025,\ttest accuracy: 0.8741\n",
      "Epoch 2,\tlr: 0.025,\ttest accuracy: 0.873\n",
      "Epoch 3,\tlr: 0.025,\ttest accuracy: 0.8811\n",
      "Epoch 4,\tlr: 0.025,\ttest accuracy: 0.8763\n",
      "Epoch 5,\tlr: 0.025,\ttest accuracy: 0.8779\n",
      "Epoch 6,\tlr: 0.025,\ttest accuracy: 0.8643\n",
      "Epoch 7,\tlr: 0.025,\ttest accuracy: 0.8748\n",
      "Epoch 8,\tlr: 0.025,\ttest accuracy: 0.8916\n",
      "Epoch 9,\tlr: 0.025,\ttest accuracy: 0.8859\n",
      "Epoch 10,\tlr: 0.025,\ttest accuracy: 0.8981\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 10\n",
    "\n",
    "opt = Optimiser(WeightDecay(5f-4), Momentum(0.025f0, 0.9f0))\n",
    "optf(e) = opt\n",
    "\n",
    "model, newpars = Lamarckism.addres_fixed(Lamarckism.pretrained(\"pretrained/backbone.onnx\"), [6,12]);\n",
    "    \n",
    "Lamarckism.train(model, nepochs, optf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did not really take us anywhere either. Perhaps some more elaborate learning rate schedule will, but this needs to work without carefully tuned hyperparameters. Lets try something else...\n",
    "\n",
    "## Experiment 4: Boost learning rate of new parameters\n",
    "One attractive thought about adding identity layers is that the new layers just fill in what is missing from the already existing layers. While it is common practice to freeze layers when fine-tuning in a transfer learning context, the evolutionary architecture search which is the motivation for this work needs to work on models which have not yet converged.\n",
    "\n",
    "As a middle road, lets try to boost the learning rate of the new parameters while keeping it small for the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.025,\ttest accuracy: 0.8745\n",
      "Epoch 2,\tlr: 0.0222,\ttest accuracy: 0.8757\n",
      "Epoch 3,\tlr: 0.0194,\ttest accuracy: 0.8874\n",
      "Epoch 4,\tlr: 0.0167,\ttest accuracy: 0.8827\n",
      "Epoch 5,\tlr: 0.0139,\ttest accuracy: 0.8923\n",
      "Epoch 6,\tlr: 0.0111,\ttest accuracy: 0.8887\n",
      "Epoch 7,\tlr: 0.00833,\ttest accuracy: 0.8983\n",
      "Epoch 8,\tlr: 0.00556,\ttest accuracy: 0.8998\n",
      "Epoch 9,\tlr: 0.00278,\ttest accuracy: 0.9012\n",
      "Epoch 10,\tlr: 0.0,\ttest accuracy: 0.902\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 10\n",
    "\n",
    "lrs = range(0.025,0.0, length=nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "\n",
    "model, newpars = Lamarckism.addres_fixed(Lamarckism.pretrained(\"pretrained/backbone.onnx\"), [6,12]);\n",
    "function optf(e)\n",
    "    opt.eta = lrs[e]\n",
    "    # Use 10 times higher learning rate for new parameters\n",
    "    boost_new = Lamarckism.BoostPars(10.0, keys(IdDict(newpars .=> 1))) # Just a clumsy way of creating an IdSet...\n",
    "    Optimiser(boost_new, WeightDecay(5f-4), opt)\n",
    "end\n",
    "    \n",
    " \n",
    "Lamarckism.train(model, nepochs, optf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm kinda glad this did not work well as it would be a bit painful to implement in NaiveGAflux.\n",
    "\n",
    "## Experiment 5: Add layers before training is complete\n",
    "In this experiement the backbone is trained for 5 epochs after which it is transformed into the resnet and trained for 15 more epochs in order to more closely model how things would look \"in the wild\". \n",
    "\n",
    "The same learning rate schedule as used initially is used throughout the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.1,\ttest accuracy: 0.3808\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.5385\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.6074\n",
      "Epoch 4,\tlr: 0.4,\ttest accuracy: 0.6524\n",
      "Epoch 5,\tlr: 0.375,\ttest accuracy: 0.6874\n",
      "Epoch 1,\tlr: 0.35,\ttest accuracy: 0.6968\n",
      "Epoch 2,\tlr: 0.325,\ttest accuracy: 0.7413\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.7693\n",
      "Epoch 4,\tlr: 0.275,\ttest accuracy: 0.7864\n",
      "Epoch 5,\tlr: 0.25,\ttest accuracy: 0.774\n",
      "Epoch 6,\tlr: 0.225,\ttest accuracy: 0.6043\n",
      "Epoch 7,\tlr: 0.2,\ttest accuracy: 0.8232\n",
      "Epoch 8,\tlr: 0.175,\ttest accuracy: 0.83\n",
      "Epoch 9,\tlr: 0.15,\ttest accuracy: 0.8603\n",
      "Epoch 10,\tlr: 0.125,\ttest accuracy: 0.8345\n",
      "Epoch 11,\tlr: 0.1,\ttest accuracy: 0.8538\n",
      "Epoch 12,\tlr: 0.075,\ttest accuracy: 0.888\n",
      "Epoch 13,\tlr: 0.05,\ttest accuracy: 0.9025\n",
      "Epoch 14,\tlr: 0.025,\ttest accuracy: 0.9092\n",
      "Epoch 15,\tlr: 0.0,\ttest accuracy: 0.9195\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 20\n",
    "\n",
    "lrs = Lamarckism.lrsched(nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "\n",
    "nepochs_first = 5\n",
    "accuracy, model = Lamarckism.train(Lamarckism.backbone(), nepochs_first, optf);\n",
    "\n",
    "model, newpars = Lamarckism.addres_fixed(model, [6,12]);\n",
    "\n",
    "accuracy, model = Lamarckism.train(model, nepochs - nepochs_first, e -> optf(e + nepochs_first));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically no impact on final accuracy! \n",
    "\n",
    "That big drop in accuracy at epoch 6 is a bit worrying though as it could easily lead to the model being discarded. It could be just normal training noise, but it looks bigger than anything encountered before. \n",
    "\n",
    "Lets see what happens the resblocks are added one by one at different epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.1,\ttest accuracy: 0.3606\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.5932\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.5659\n",
      "Epoch 4,\tlr: 0.4,\ttest accuracy: 0.6504\n",
      "Epoch 5,\tlr: 0.375,\ttest accuracy: 0.5672\n",
      "Epoch 1,\tlr: 0.35,\ttest accuracy: 0.7008\n",
      "Epoch 2,\tlr: 0.325,\ttest accuracy: 0.6895\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.7082\n",
      "Epoch 4,\tlr: 0.275,\ttest accuracy: 0.7521\n",
      "Epoch 5,\tlr: 0.25,\ttest accuracy: 0.781\n",
      "Epoch 1,\tlr: 0.225,\ttest accuracy: 0.8096\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.8236\n",
      "Epoch 3,\tlr: 0.175,\ttest accuracy: 0.8353\n",
      "Epoch 4,\tlr: 0.15,\ttest accuracy: 0.8311\n",
      "Epoch 5,\tlr: 0.125,\ttest accuracy: 0.8387\n",
      "Epoch 6,\tlr: 0.1,\ttest accuracy: 0.8516\n",
      "Epoch 7,\tlr: 0.075,\ttest accuracy: 0.8856\n",
      "Epoch 8,\tlr: 0.05,\ttest accuracy: 0.8965\n",
      "Epoch 9,\tlr: 0.025,\ttest accuracy: 0.9137\n",
      "Epoch 10,\tlr: 0.0,\ttest accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 20\n",
    "\n",
    "lrs = Lamarckism.lrsched(nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "\n",
    "accuracy, model = Lamarckism.train(Lamarckism.backbone(), 5, optf);\n",
    "\n",
    "model, newpars = Lamarckism.addres_fixed(model, [12]);\n",
    "\n",
    "accuracy, model = Lamarckism.train(model, 5, e -> optf(e + 5));\n",
    "\n",
    "model, newpars = Lamarckism.addres_fixed(model, [6]);\n",
    "\n",
    "accuracy, model = Lamarckism.train(model, 10, e -> optf(e + 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work as well!\n",
    "\n",
    "This run did not display the same big drop in accuracy, but it is not possible to say if this is because we did smaller changes or if it is just happened randomly from just two single runs. I guess doing more runs could help shed some light on this but lets save that for a rainy day.\n",
    "\n",
    "Lets see if doing a big change relatively late has any negative impact on performance. Same experiment as above but add both resblocks after 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.1,\ttest accuracy: 0.3651\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.4769\n",
      "Epoch 3,\tlr: 0.3,\ttest accuracy: 0.646\n",
      "Epoch 4,\tlr: 0.4,\ttest accuracy: 0.646\n",
      "Epoch 5,\tlr: 0.375,\ttest accuracy: 0.7023\n",
      "Epoch 6,\tlr: 0.35,\ttest accuracy: 0.7176\n",
      "Epoch 7,\tlr: 0.325,\ttest accuracy: 0.7627\n",
      "Epoch 8,\tlr: 0.3,\ttest accuracy: 0.7342\n",
      "Epoch 9,\tlr: 0.275,\ttest accuracy: 0.6961\n",
      "Epoch 10,\tlr: 0.25,\ttest accuracy: 0.7649\n",
      "Epoch 1,\tlr: 0.225,\ttest accuracy: 0.7719\n",
      "Epoch 2,\tlr: 0.2,\ttest accuracy: 0.8174\n",
      "Epoch 3,\tlr: 0.175,\ttest accuracy: 0.8033\n",
      "Epoch 4,\tlr: 0.15,\ttest accuracy: 0.8374\n",
      "Epoch 5,\tlr: 0.125,\ttest accuracy: 0.8307\n",
      "Epoch 6,\tlr: 0.1,\ttest accuracy: 0.8237\n",
      "Epoch 7,\tlr: 0.075,\ttest accuracy: 0.8594\n",
      "Epoch 8,\tlr: 0.05,\ttest accuracy: 0.8896\n",
      "Epoch 9,\tlr: 0.025,\ttest accuracy: 0.903\n",
      "Epoch 10,\tlr: 0.0,\ttest accuracy: 0.9074\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 20\n",
    "\n",
    "lrs = Lamarckism.lrsched(nepochs)\n",
    "opt = Momentum(0.1f0, 0.9f0)\n",
    "function optf(e)\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "\n",
    "nepochs_first = 10\n",
    "accuracy, model = Lamarckism.train(Lamarckism.backbone(), nepochs_first, optf);\n",
    "\n",
    "model, newpars = Lamarckism.addres_fixed(model, [6,12]);\n",
    "\n",
    "accuracy, model = Lamarckism.train(model, nepochs - nepochs_first, e -> optf(e + nepochs_first));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a slight decrease in accuracy although it is hard to tell if it is significant from a single run.\n",
    "\n",
    "## Experiment 6: Learnable fade-in\n",
    "The previous experiements used a fixed equal weighting between the input to a residual block and its output. Is it possible to do better if this input is learnable?\n",
    "\n",
    "Lets try to fine tune the pre-trained model when the new layers are weighted quite low compared to the input to the resblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\tlr: 0.05,\ttest accuracy: 0.8548\n",
      "Epoch 2,\tlr: 0.0444,\ttest accuracy: 0.8728\n",
      "Epoch 3,\tlr: 0.0389,\ttest accuracy: 0.8697\n",
      "Epoch 4,\tlr: 0.0333,\ttest accuracy: 0.8772\n",
      "Epoch 5,\tlr: 0.0278,\ttest accuracy: 0.874\n",
      "Epoch 6,\tlr: 0.0222,\ttest accuracy: 0.8805\n",
      "Epoch 7,\tlr: 0.0167,\ttest accuracy: 0.8837\n",
      "Epoch 8,\tlr: 0.0111,\ttest accuracy: 0.904\n",
      "Epoch 9,\tlr: 0.00556,\ttest accuracy: 0.9056\n",
      "Epoch 10,\tlr: 0.0,\ttest accuracy: 0.9091\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(1)\n",
    "nepochs = 10\n",
    "\n",
    "lrs = range(0.05,0.0, length=nepochs)\n",
    "opt = Momentum(lrs[1], 0.9f0)\n",
    "function optf(e)\n",
    "    opt.eta = lrs[e]\n",
    "    Optimiser(WeightDecay(5f-4), opt)\n",
    "end\n",
    "    \n",
    "model, newpars = Lamarckism.addres_learnable(Lamarckism.pretrained(\"pretrained/backbone.onnx\"), [6,12];  λ=0.9);\n",
    "    \n",
    "Lamarckism.train(model, nepochs, optf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing revolutionary it seems. More experiments needed perhaps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
